import { GoogleGenerativeAI } from "@google/generative-ai";
import expressAsyncHandler from "express-async-handler";
import Message from "../models/Message.js";
import GroupMessage from "../models/GroupMessage.js";

// --- Configuration & Initialization ---

if (!process.env.GEMINI_API_KEY) {
    console.error("âŒ GEMINI_API_KEY is missing in environment variables.");
}

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// Using 'gemini-1.5-flash' is often more stable, but adhering to your specific model choice:
const model = genAI.getGenerativeModel({ model: "gemini-flash-latest" });

/**
 * Utility: List available Gemini models on startup for debugging.
 * Non-blocking operation.
 */
async function listAvailableModels() {
    try {
        const response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models?key=${process.env.GEMINI_API_KEY}`
        );
        const data = await response.json();
        console.log("ğŸ“‹ Available Models:", data.models?.map((m) => m.name));
    } catch (error) {
        console.warn("âš ï¸ Failed to list models:", error.message);
    }
}
// Execute on load
listAvailableModels();

// --- Controllers ---

/**
 * @desc    Summarize chat history using Google Gemini
 * @route   POST /api/ai/summarize
 * @access  Private
 */
export const summarizeChat = expressAsyncHandler(async (req, res) => {
    const { chatId, isGroup } = req.body;

    // 1. Fetch Messages
    let messages = [];
    const limit = 50;

    try {
        if (isGroup) {
            messages = await GroupMessage.find({ group: chatId })
                .sort({ createdAt: -1 })
                .limit(limit)
                .populate("sender", "full_name");
        } else {
            // Fetch 1-on-1 messages strictly adhering to original logic
            messages = await Message.find({
                $or: [{ sender: chatId }, { receiver: chatId }],
            })
                .sort({ createdAt: -1 })
                .limit(limit)
                .populate("sender", "full_name");
        }
    } catch (dbError) {
        res.status(500);
        throw new Error("Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.");
    }

    // 2. Validate Content Availability
    if (!messages || messages.length < 3) {
        res.status(400);
        throw new Error("Ø§Ù„Ø±Ø³Ø§ÙŠÙ„ Ù‚Ù„ÙŠÙ„Ø© Ø£ÙˆÙŠ Ø¹Ø´Ø§Ù† ØªØªÙ„Ø®Øµ!");
    }

    // 3. Prepare Context for AI
    // Reverse to chronological order (Oldest -> Newest) for the LLM context
    const conversationText = messages
        .reverse()
        .map((msg) => {
            const senderName = msg.sender?.full_name || "Unknown";
            const textContent = msg.text || "[Media/Deleted]";
            return `${senderName}: ${textContent}`;
        })
        .join("\n");

    // 4. Generate Summary
    try {
        const prompt = `
            Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø´Ø§Øª Ù…ØµØ±ÙŠ.
            Ø¯ÙŠ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø´Ø®Ø§Øµ:
            
            ${conversationText}
            
            Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ù„Ø®Øµ Ø§Ù„Ù„ÙŠ Ø­ØµÙ„ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¯ÙŠ ÙÙŠ Ø´ÙƒÙ„ Ù†Ù‚Ø§Ø· Ø¨Ø³ÙŠØ·Ø© (Bullets) Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¹Ø§Ù…ÙŠØ©.
            Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù„ÙŠ Ø®Ø¯ÙˆÙ‡Ø§ Ø£Ùˆ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø£Ùˆ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù„ÙŠ Ø§ØªØ­Ù„Øª.
            Ù…ØªØ²ÙˆØ¯Ø´ ÙƒÙ„Ø§Ù… Ù…Ù† Ø¹Ù†Ø¯ÙƒØŒ Ø®Ù„ÙŠÙƒ ÙÙŠ Ø§Ù„Ù…ÙÙŠØ¯.
        `;

        const result = await model.generateContent(prompt);
        const response = await result.response;
        const summary = response.text();

        res.status(200).json({ success: true, summary });

    } catch (error) {
        console.error("Gemini Summarization Error:", error);
        res.status(500);
        throw new Error("ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªÙ„Ø®ÙŠØµØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„");
    }
});

/**
 * @desc    Check content safety (Moderation) using Gemini
 * @route   POST /api/ai/moderate
 * @access  Private
 */
export const moderateContent = expressAsyncHandler(async (req, res) => {
    const { text } = req.body;

    // Fail-safe: If no text, consider it safe to avoid blocking empty states
    if (!text || typeof text !== 'string') {
        return res.status(200).json({ safe: true });
    }

    try {
        const prompt = `
            Ù‡Ù„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†Ù Ø´Ø¯ÙŠØ¯ØŒ ÙƒØ±Ø§Ù‡ÙŠØ©ØŒ ØªÙ†Ù…Ø± Ù‚Ø§Ø³ÙŠØŒ Ø£Ùˆ Ø£Ù„ÙØ§Ø¸ Ø¨Ø°ÙŠØ¦Ø© Ø¬Ø¯Ø§Ù‹ØŸ
            Ø§Ù„Ù†Øµ: "${text}"
            
            Ø¬Ø§ÙˆØ¨ Ø¨ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·: "SAFE" Ø£Ùˆ "UNSAFE".
            Ù„Ùˆ Ø§Ù„Ù†Øµ Ø¹Ø§Ø¯ÙŠ Ø£Ùˆ Ù‡Ø²Ø§Ø± Ø¨Ø³ÙŠØ· Ù‚ÙˆÙ„ SAFE.
        `;

        const result = await model.generateContent(prompt);
        const response = await result.response;

        // Normalize response to ensure robust checking
        const decision = response.text().trim().toUpperCase();
        const isSafe = decision.includes("SAFE");

        res.status(200).json({
            safe: isSafe,
            categories: isSafe ? {} : { flag: "content_policy_violation" },
        });

    } catch (error) {
        console.error("Gemini Moderation Error:", error);
        // Fail-open strategy: In case of AI downtime, do not block user messages
        res.status(200).json({ safe: true });
    }
});